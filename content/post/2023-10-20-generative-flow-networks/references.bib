
@misc{bengio_gflownet_2023,
	title = {{GFlowNet} Foundations},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks ({GFlowNets}) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of {GFlowNets}. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. {GFlowNets} amortize the work typically done by computationally expensive {MCMC} methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	number = {{arXiv}:2111.09266},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	urldate = {2023-09-20},
	date = {2023-07-10},
	eprinttype = {arxiv},
	eprint = {2111.09266 [cs, stat]},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/andrej/myStuff/Zotero_Library/storage/GWEDRXW9/2111.html:text/html;Bengio et al. - 2023 - GFlowNet Foundations.pdf:/home/andrej/myStuff/Zotero_Library/storage/K5CRDUCS/Bengio et al. - 2023 - GFlowNet Foundations.pdf:application/pdf},
}

@inproceedings{hu_gflownet-em_2023,
	title = {{GFlowNet}-{EM} for Learning Compositional Latent Variable Models},
	url = {https://proceedings.mlr.press/v202/hu23c.html},
	abstract = {Latent variable models ({LVMs}) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectation-maximization ({EM}), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of {GFlowNets}, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training {GFlowNets} to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, {GFlowNet}-{EM}, enables the training of expressive {LVMs} with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders ({VAEs}) without conditional independence enforced in the encoder.},
	eventtitle = {International Conference on Machine Learning},
	pages = {13528--13549},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Hu, Edward J. and Malkin, Nikolay and Jain, Moksh and Everett, Katie E. and Graikos, Alexandros and Bengio, Yoshua},
	urldate = {2023-09-20},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	keywords = {\_tablet},
	file = {Hu et al. - 2023 - GFlowNet-EM for Learning Compositional Latent Vari.pdf:/home/andrej/myStuff/Zotero_Library/storage/RVBZZBIU/Hu et al. - 2023 - GFlowNet-EM for Learning Compositional Latent Vari.pdf:application/pdf},
}

@misc{zhang_unifying_2023,
	title = {Unifying Generative Models with {GFlowNets} and Beyond},
	url = {http://arxiv.org/abs/2209.02606},
	doi = {10.48550/arXiv.2209.02606},
	abstract = {There are many frameworks for deep generative modeling, each often presented with their own specific training algorithms and inference methods. Here, we demonstrate the connections between existing deep generative models and the recently introduced {GFlowNet} framework, a probabilistic inference machine which treats sampling as a decision-making process. This analysis sheds light on their overlapping traits and provides a unifying viewpoint through the lens of learning with Markovian trajectories. Our framework provides a means for unifying training and inference algorithms, and provides a route to shine a unifying light over many generative models. Beyond this, we provide a practical and experimentally verified recipe for improving generative modeling with insights from the {GFlowNet} perspective.},
	number = {{arXiv}:2209.02606},
	publisher = {{arXiv}},
	author = {Zhang, Dinghuai and Chen, Ricky T. Q. and Malkin, Nikolay and Bengio, Yoshua},
	urldate = {2023-09-20},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2209.02606 [cs, stat]},
	keywords = {\_tablet, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/andrej/myStuff/Zotero_Library/storage/HXK8K6XH/2209.html:text/html;Zhang et al. - 2023 - Unifying Generative Models with GFlowNets and Beyo.pdf:/home/andrej/myStuff/Zotero_Library/storage/ZTHGQUPR/Zhang et al. - 2023 - Unifying Generative Models with GFlowNets and Beyo.pdf:application/pdf},
}

@inproceedings{bengio_flow_2021,
	title = {Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/e614f646836aaed9f89ce58e837e2310-Abstract.html},
	abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While {MCMC} methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose {GFlowNet}, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of {GFlowNet} on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
	pages = {27381--27394},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
	urldate = {2023-10-15},
	date = {2021},
	keywords = {\_tablet},
	file = {Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf:/home/andrej/myStuff/Zotero_Library/storage/2UCBUJS6/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf:application/pdf},
}
