[{"authors":null,"categories":null,"content":"I am a Ph.D. student at the Department of Statistics, University of Michigan.\nPreviously, I did research at UC Berkeley, where I was part of Project CETI, which aims to decipher sperm whale communication with machine learning.\nYou can get the source code of this website at https://github.com/andleb/blog.\n Download my resumé. --  Master\u0026#39;s thesis (Diploma) in Mathematical Physics: Time-dependent current through a quantum dot in the presence of a voltage probe (in Slovene) [^1]. [^1]: NOTE: The document significantly compresses the theoretical background of the thesis, which is to be found in the referenced works, since it falls outside of the Diploma program and recommended thesis format. The focus of the work is on the empirical results, which can be appreciated even with the compressed background.  -- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://andleb.netlify.app/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a Ph.D. student at the Department of Statistics, University of Michigan.\nPreviously, I did research at UC Berkeley, where I was part of Project CETI, which aims to decipher sperm whale communication with machine learning.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"I recently helped present our work (together with Prof. Gašper Beguš, paper) on the usage of deep generative models for discovering the building blocks of sperm whale communication at the Decoding Communication in Nonhuman Species symposium hosted at the Simons Institute for the Theory of Computing, UC Berkeley:\n ","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689140884,"objectID":"a10d7b0bde6f5b851af89b7c201d32b3","permalink":"https://andleb.netlify.app/post/presentation-of-our-work-on-whale-communication-at-the-simons-institute/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/post/presentation-of-our-work-on-whale-communication-at-the-simons-institute/","section":"post","summary":"I recently helped present our work (together with Prof. Gašper Beguš, paper) on the usage of deep generative models for discovering the building blocks of sperm whale communication at the Decoding Communication in Nonhuman Species symposium hosted at the Simons Institute for the Theory of Computing, UC Berkeley:","tags":["Deep Learning","Generative models","Causal Inference"],"title":"A talk on our work on whale communication at the Simons Institute","type":"post"},{"authors":["Gašper Beguš","Andrej Leban","Shane Gero"],"categories":null,"content":"","date":1679270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679270400,"objectID":"c128236c56c887b9c4dcfda67c83df21","permalink":"https://andleb.netlify.app/publication/causalceti/","publishdate":"2023-03-20T00:00:00Z","relpermalink":"/publication/causalceti/","section":"publication","summary":"In the paper, we combine approaches that help discover how known properties of human language are learned by generative models when trained on labeled speech audio data with methodology inspired by causal inference and apply it to the communication system of sperm whales, for which we do not have any such ground truth. With this, we can propose what components of their communication might serve as the carriers of meaning, not only giving credence to existing theories but also suggesting additional ways the whales might encode information.","tags":[],"title":"Approaching an unknown communication system by latent space exploration and causal inference","type":"publication"},{"authors":[],"categories":[],"content":"We have a pre-print out based on our work with Project CETI, an initiative to decipher sperm whale communication using machine learning.\nIn the paper, we combine approaches that help discover how known properties of human language are learned by generative models when trained on labeled speech audio data with methodology inspired by causal inference and apply it to the communication system of sperm whales, for which we do not have any such ground truth. With this, we can propose what components of their communication might serve as the carriers of meaning, not only giving credence to existing theories but also suggesting additional ways the whales might encode information.\n","date":1679270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679377527,"objectID":"936ba00ae71f527e8997db34ea8e15ef","permalink":"https://andleb.netlify.app/post/new-publication-approaching-an-unknown-communication-system/","publishdate":"2023-03-20T00:00:00Z","relpermalink":"/post/new-publication-approaching-an-unknown-communication-system/","section":"post","summary":"We have a pre-print out based on our work with Project CETI, an initiative to decipher sperm whale communication using machine learning.\nIn the paper, we combine approaches that help discover how known properties of human language are learned by generative models when trained on labeled speech audio data with methodology inspired by causal inference and apply it to the communication system of sperm whales, for which we do not have any such ground truth.","tags":["Statistics","Deep Learning","Causal Inference"],"title":"New publication: Approaching an unknown communication system","type":"post"},{"authors":[],"categories":[],"content":" Table of Contents  Introduction Diffusion models seem to have taken the world by storm due to their amazing generative powers. That is not their only advantage, however. With a bit of a tongue in the proverbial cheek, statistical methods can be traditionally classified as either inflexible (classical stats), computationally expensive (MCMC), or non-analytical (boosted trees). From this perspective, diffusion models are a significant outlier since they are extremely flexible, provide access to the full posterior (and conditional) distributions, and are computationally less expensive than many of the competing methods.\n General structure An overview of generative models (source)  Diffusion models are composed of two separate processes, the forward and the backward.\nForward diffusion In general - a diffusion process can be characterized by a Markov diffusion kernel given a final state \\(\\pi(x)\\):\n\\[\\begin{equation} q(x_t | x_{t-1}) = T_\\pi(x_t | x_{t-1}; \\beta_t) \\end{equation}\\] Since it’s a length-one Markov process, we have for the full joint: \\[\\begin{equation} q(x_{t=0:T} ) = q(x_0) \\prod_{i=1}^T q(x_i | x_{i-1}) \\end{equation}\\]\nUsually, we use a Gaussian diffusion, for which the posterior is closed form (c.f. the 1-step update in Kalman filtering):\n\\[\\begin{equation} q(x_t | x_{t-1}) = \\mathcal{N}(\\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t \\mathbf{I}) \\end{equation}\\] \\(\\beta_i\\)s are called the variance schedule, with usually \\(\\beta_1 \u0026lt; \\beta_2 \u0026lt; ... \u0026lt; \\beta_T\\).\nMoreover, due to the property of the Gaussian distribution:\n\\[\\alpha_t := 1 - \\beta_t\\]\n\\[\\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_t\\]\n\\[\\begin{equation} q(x_t | x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha_t}}~x_0, (1 - \\alpha_t) \\mathbf{I}), \\end{equation}\\] meaning any state in the forward process can be expressed knowing just the initial one and the variance schedule. In general, the theoretical underpinning of Langevin dynamics guarantees any smooth distribution can be corrupted into Gaussian noise, meaning the initial distribution can be almost arbitrarily complex, giving the model its expressive power.\n Reverse diffusion The reverse process is characterized by a new transition probability \\(p(x)\\). Its starting point is the stationary distribution at the final time \\(T\\):\n\\[\\begin{equation} \\pi(x_T) := p(x_T) \\end{equation}\\] Like before:\n\\[\\begin{equation} p(x_{t=T:0}) = p(x_T) \\prod_{i=T-1}^1 p(x_i+1 | x_{i}) \\end{equation}\\] For Gaussian (and binomial), the joint is still in the same family; however, there is no closed-form for the parameters \\(\\mu\\) and \\(\\Sigma\\). These need to be estimated, in this case via neural networks.\nA natural training target is the likelihood of the original data \\(p(x_0)\\) as given by the learned reverse process’ distribution, obtained by marginalizing the full joint: \\[\\begin{equation} p(x_0) = \\int_{\\mathcal{X}} p(x_{t=T:0}) ~ dx_1 .... dx_T \\end{equation}\\]\nThis is computationally intractable! A trick is to use annealed importance sampling - comparing the relative probability of the backward - \\(p\\) - and the forward trajectories \\(q\\), where the latter are known in closed form:\n\\[\\begin{equation} p(x_0) = \\int_{\\mathcal{X}} q(x_{t=1:T}) ~p(x_T)~ \\prod_{t=1}^T \\frac{p(x_{t-1}|x_t)}{q(x_{t}|x_{t-1})} ~ dx_1 .... dx_T \\end{equation}\\] In the limit of very small \\(\\beta\\), both directions become the same.\n  Training We maximize the expected log likelihood of the original data \\(p(x_0)\\) under the original true distribution \\(q(x_0)\\):\n\\[\\begin{aligned} \u0026amp; L(p, q) = \\mathbb{E}_{q(x_0)} [p(x_0)] = \\int_{\\mathcal{X}} q(x_0) ~ \\log p(x_T)~ dx_0\\\\ \u0026amp; = -~\\mathbb{E}_{q} \\left[ \\underbrace{D( q(x_T | x_0) || p(x_T))}_{L_T} + \\underbrace{\\sum_{t=2}^T D( q(x_{t-1} | x_t, x_0) || p(x_{t-1} || x_t))}_{L_{t-1}} - \\underbrace{\\log p(x_0 | x+1)}_{L_0} \\right] \\end{aligned}\\] It can be lower bounded by a closed-form expression:\n\\[\\begin{aligned} L \\geq K = - \\sum_{t=2}^T \\int q(x_0, x_t) ~ D(q(x_{t-1}|x_{t}, x_{0}) ~||~ p(x_{t-1}|x_{t}))~dx_0,~dx_t\\\\ + H_q(X_T|X_0) - H_q(X_1|X_0) - H_p(X_T),\u0026amp; \\end{aligned}\\] where \\(D\\) is the K-L divergence and H denotes the (conditional) entropies. Hence maximizing the latter maximizes the former.\nAdditionally, conditioning the forward process posteriors on \\(x_0\\) gives us the closed-forms\n\\[\\begin{equation} q(x_{t-1}|x_{t}, x_{0}) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t \\mathbf{I}) \\end{equation}\\] \\[\\tilde{\\mu}_t := \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t}~x_0 + \\frac{\\sqrt{\\bar{\\alpha}_{t}} (1 - \\bar{\\alpha}_{t})}{1 - \\bar{\\alpha}_t}~x_t\\]\n\\[\\tilde{\\beta}_t := \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\\]\nThe goal of training is therefore to estimate the reverse Markov transition densities \\(p(x_{t-1}|x_t)\\): \\[\\hat{p}(x_{t-1}|x_t) = \\underset{p}{\\operatorname{argmax}} K\\]\nAs mentioned, in the case of the Gaussian and binomial, the reverse process stays in the same family, therefore the task amounts to estimating the parameters.\nVariance schedule Since …","date":1664668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665376560,"objectID":"300d30b09ac19d1f546eed75c567b6d9","permalink":"https://andleb.netlify.app/post/diffusion-models/","publishdate":"2022-10-02T00:00:00Z","relpermalink":"/post/diffusion-models/","section":"post","summary":"A brief introduction to diffusion models","tags":["Deep Learning","Statistics","Diffusion"],"title":"Diffusion models: an introduction","type":"post"},{"authors":[],"categories":[],"content":"Given that the semester’s over, I have finally gotten around to updating the site.\nWe have a pre-print out based on the work I did during my internship last summer. It examines the problem of inferring the characteristics of different people involved in the process of translating and reviewing text by using Hierarchical Bayesian Models. Specifically, I came to the idea of using zero-inflated and fat-tailed distributions to model the number of mistakes; this was influenced both by the statistical properties of the raw data and by the peculiarities of the process of translation.\nAdditionally, I have made public the repositories for two of the final projects I did this year at UC Berkeley:\n ganIsing 230Afinal  (I might make public more in the future…)\n","date":1652572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652648557,"objectID":"4656fd21d95789ba9e1b68a54654b529","permalink":"https://andleb.netlify.app/post/new-publication-projects/","publishdate":"2022-05-15T00:00:00Z","relpermalink":"/post/new-publication-projects/","section":"post","summary":"Given that the semester’s over, I have finally gotten around to updating the site.\nWe have a pre-print out based on the work I did during my internship last summer. It examines the problem of inferring the characteristics of different people involved in the process of translating and reviewing text by using Hierarchical Bayesian Models.","tags":["Statistics","Deep Learning"],"title":"New publication, projects","type":"post"},{"authors":["Marco Miccheli","Andrej Leban","Andrea Tacchella","Andrea Zaccaria","Dario Mazzilli","Sébastien Bratières"],"categories":null,"content":"NOTE: This paper stems from the work I did as a Machine Learning intern at Translated in Rome during the summer of 2021.\n","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649721600,"objectID":"b4e8f30242bcad7f8ced6758ee50201c","permalink":"https://andleb.netlify.app/publication/bayesiantranslators/","publishdate":"2022-04-12T00:00:00Z","relpermalink":"/publication/bayesiantranslators/","section":"publication","summary":"Translation Quality Assessment (TQA) is a process conducted by human translators and is widely used, both for estimating the performance of (increasingly used) Machine Translation, and for finding an agreement between translation providers and their customers. While translation scholars are aware of the importance of having a reliable way to conduct the TQA process, it seems that there is limited literature that tackles the issue of reliability with a quantitative approach. In this work, we consider the TQA as a complex process from the point of view of physics of complex systems and approach the reliability issue from the Bayesian paradigm. Using a dataset of translation quality evaluations (in the form of error annotations), produced entirely by the Professional Translation Service Provider Translated SRL, we compare two Bayesian models that parameterise the following features involved in the TQA process: the translation difficulty, the characteristics of the translators involved in producing the translation, and of those assessing its quality - the reviewers. We validate the models in an unsupervised setting and show that it is possible to get meaningful insights into translators even with just one review per translation; subsequently, we extract information like translators’ skills and reviewers’ strictness, as well as their consistency in their respective roles. Using this, we show that the reliability of reviewers cannot be taken for granted even in the case of expert translators: a translator’s expertise can induce a cognitive bias when reviewing a translation produced by another translator. The most expert translators, however, are characterised by the highest level of consistency, both in translating and in assessing the translation quality.","tags":[],"title":"A Bayesian approach to translators' reliability assessment","type":"publication"},{"authors":[],"categories":[],"content":" NOTE: This is a migration of an old post from my previous blog. In this post I’d like to briefly sketch my personal setup for git repositories, focusing on their backup.\nRepository architecture overview Generally, each repository on my desktop system will have at least two remotes. The first, the ubiquitous origin, will be located inside an encrypted folder that is mounted inside the Dropbox directory and hence automatically synced to the cloud. This setup is replicated across all my other machines; its setup could be a topic of a future post.\nThe second, which I usually name backup, is located on a separate hard drive reserved for this purpose. It mirrors the whole repository, which is achieved using:\ngit push --mirror backup on the first-time push.\n Using git aliases to sync the backup on each push Since this repository is (hopefully) never pulled from, but only kept up to date with an acceptable state of the working repository, it is therefore convenient to avoid having to push to it manually. For this purpose I set up a git alias in my .gitconfig, which tries to push to all remotes of a particular repository. In the case of the aforementioned origin, this command will naturally fail on the occasions where the paths have diverged. This I let happen and move on, since the motto is “fast–forward if possible”. Due to the nature of the backup repository, however, pushing to it should never fail:\n[alias] ... pushallbr = !\u0026#34;git remote -v | grep -v \u0026#39;github\u0026#39; | grep -Eo ^\\\\w+\u0026#39;|\\ xargs -L1 -I R git push R\u0026#34; ... “pushallbr” stands for “push all branches”, one can of course pick a different alias. Note that I filter out github repositories explicitly (the -v flag in grep inverts the match). The purpose of this command is just syncing the local repositories, for the ones on github I want to retain manual control. In my workflow I usually use this command over the common push origin master and thus sync the backup “automatically”.\n Syncing all sub-directories Having set up our backup repository and added an alias that syncs it, an useful thing to have would be a shell script that descends into a hierarchy of repositories and tries to sync them all:\n#! /bin/bash {find . -maxdepth 8 -type d -exec bash -c \u0026#34;git --git-dir={}/.git --work-tree=$PWD/{} pushallbr\u0026#34; \\;} 2\u0026gt;/dev/null  Things to note:\n Since we expect the command to fail in some (or many) cases without adverse side effects, we ignore the error messages by redirecting them to /dev/null\n To be able to use a complex git alias within a find ... -exec command, one must invoke the whole thing as an inline script via bash -c \u0026#34;...\u0026#34;\n   ","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641432807,"objectID":"447fe51a8b4c27179be23743c60c26ac","permalink":"https://andleb.netlify.app/post/how-to-backup-git-repositories/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/post/how-to-backup-git-repositories/","section":"post","summary":"NOTE: This is a migration of an old post from my previous blog. In this post I’d like to briefly sketch my personal setup for git repositories, focusing on their backup.","tags":["bash","Dropbox","Git","GitHub","Linux"],"title":"How to backup git repositories","type":"post"},{"authors":[],"categories":[],"content":" NOTE: This is a migration of an old post from my previous blog. Recently, I’ve been playing around with some competitions on Kaggle. Given that an inescapable fact of Machine Learning is Feature Selection, I’ve been finding myself in the situation of having to call a dozen or more functions that add synthetic features, infer missing values, etc., on the same Pandas DataFrame.\nThe following code snippet will call every function in its .py file but itself on the object, using tail recursion (nested helper function recCall):\nimport inspect import sys import pandas as pd def addAllFeatures(data: pd.DataFrame): currFunc = inspect.getframeinfo(inspect.currentframe()).function functions = [obj for name, obj in inspect.getmembers(sys.modules[__name__]) if (inspect.isfunction(obj) and name != currFunc)] def recCall(modifiedData, remFuncs): if len(remFuncs) == 0: return modifiedData return recCall(remFuncs[0](modifiedData), remFuncs[1:]) return recCall(data, functions) For this specific example, the general form of the feature-adding functions is:\ndef addFeature(data: pd.DataFrame, *args, **kwargs): # add the feature ... return data Also notable is the fact that the data gets modified between calls of the recursion; if your features depend on each other, then the functions list would need to be in the correct dependency order. Determining this can, however, quickly become non-trivial.\n","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641435432,"objectID":"b273328f6223bc28325ae4d43f09efff","permalink":"https://andleb.netlify.app/post/how-to-call-all-the-functions-in-a-python-file-on-an-object/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/post/how-to-call-all-the-functions-in-a-python-file-on-an-object/","section":"post","summary":"NOTE: This is a migration of an old post from my previous blog. Recently, I’ve been playing around with some competitions on Kaggle. Given that an inescapable fact of Machine Learning is Feature Selection, I’ve been finding myself in the situation of having to call a dozen or more functions that add synthetic features, infer missing values, etc.","tags":["Data Science","Python","Reflection","Machine Learning"],"title":"How to call all the functions in a Python file on an object","type":"post"},{"authors":[],"categories":[],"content":" NOTE: This is a migration of an old post from my previous blog. Something extremely cool happened to two of my GitHub repositories (please pardon the pun). Namely, they were included in Github’s Arctic Vault Archive program.\nIn short, GitHub has partnered with a Norwegian company to write some of the repositories to special, durable film reels and deposit them 250 meters underground in an abandoned coal mine in the remote Svalbard islands.\nAll in all, about 200 3500-foot (1km) reels of film were thus deposited.\nGiven how often we hear the familiar complaint of the supposed transience of modern products, it is uplifting to see a company take a concrete action to remedy this. As for myself, I am deeply honored to have something that I created preserved for posterity.\n ","date":1641340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641435670,"objectID":"2ee885fc896a5eabdfed3a3b2da6a4d1","permalink":"https://andleb.netlify.app/post/the-github-s-arctic-code-vault/","publishdate":"2022-01-05T00:00:00Z","relpermalink":"/post/the-github-s-arctic-code-vault/","section":"post","summary":"NOTE: This is a migration of an old post from my previous blog. Something extremely cool happened to two of my GitHub repositories (please pardon the pun). Namely, they were included in Github’s Arctic Vault Archive program.","tags":["GitHub"],"title":"The GitHub’s Arctic Code Vault","type":"post"},{"authors":[],"categories":[],"content":" Since Wordpress was just annoying enough for typesetting math \u0026amp; code to dissuade me from posting regularly, I have decided to migrate to blogdown. I will, however, move over some of the posts there.\n","date":1641254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641362343,"objectID":"c5c8deff75030354d9ca8078cf50df70","permalink":"https://andleb.netlify.app/post/new-blog-format/","publishdate":"2022-01-04T00:00:00Z","relpermalink":"/post/new-blog-format/","section":"post","summary":"Since Wordpress was just annoying enough for typesetting math \u0026 code to dissuade me from posting regularly, I have decided to migrate to blogdown. I will, however, move over some of the posts there.","tags":[],"title":"New blog format","type":"post"},{"authors":null,"categories":null,"content":"This is my final project for STAT241 / CS281A: Statistical Learning Theory at UC Berkeley in Fall 2021.\nGiven that I’ve gotten out of Physics just about when the Deep Learning “revolution” hit the field, it was a great reason to try in person how generative adversarial networks can replicate results obtained by time-consuming Monte Carlo simulation, something I am deeply familiar with.\nThe goal of this project was two-fold: to examine the approach on a classic example - the 2D Ising model, and to provide a brief (the final report linked above was limited to 10 pages) overview of the use of GANs in general.\n","date":1639526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639526400,"objectID":"4ea1e02ad5d3639f2b263e37d305ca12","permalink":"https://andleb.netlify.app/project/ganising/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/project/ganising/","section":"project","summary":"Solving the 2D Ising model using GANs","tags":["Deep Learning","Monte Carlo","Physics"],"title":"ganIsing","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://andleb.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://andleb.netlify.app/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"License","type":"page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://andleb.netlify.app/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"License","type":"page"},{"authors":["Andrej Leban"],"categories":null,"content":"","date":1457136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1457136000,"objectID":"fcbe192c0737997b64c00ec5621cc467","permalink":"https://andleb.netlify.app/publication/dipoma/","publishdate":"2023-07-20T00:00:00Z","relpermalink":"/publication/dipoma/","section":"publication","summary":"Please see the accompanying [projects page](content/project/diploma/).","tags":["Mesoscopic transport","Nanophysics","NEGF"],"title":"Time-dependent current through a quantum dot in the presence of a voltage probe","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://andleb.netlify.app/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://andleb.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://andleb.netlify.app/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"This is my final project for STAT230A: Linear Models at UC Berkeley in Spring 2022, done jointly with Isaac Schmidt.\nThe goal was to replicate the findings of Michalopoulos: The Origins of Ethnolinguistic Diversity. The project took an additional dimension since we couldn’t obtain some of the data used in the paper - the WLMS dataset, prompting us to recreate a portion of the paper using the GREG dataset.\nThe main finding of the paper, which also bore out in our re-analysis using the alternative dataset, is that the variation in the elevation and in the land quality (see the map above) are the two most decisive factors in driving a region’s ethnolinguistic diversity.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ba959433589c53a9c2fa13c01e45828f","permalink":"https://andleb.netlify.app/project/230afinal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/230afinal/","section":"project","summary":"Replication of Michalopoulos: The Origins of Ethnolinguistic Diversity","tags":["Statistics"],"title":"230A final","type":"project"},{"authors":null,"categories":null,"content":"A few years back, I undertook a significant study of mathematical finance. I chiefly followed M. Joshi’s Concepts and Practice of Mathematical Finance and More Mathematical Finance. Since the companion practical textbook, C++ Design Patterns and Derivatives Pricing, was based on outdated, pre-C++11 paradigms, and the author has sadly passed away, I decided to rewrite the latter in modern C++. The goal was to keep the original spirit while adding advanced functionality. I hope that other people studying from the same sources find the project useful, and I have thus, in a small way, helped keep the book relevant.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5a162cba7aefeae23295ef023175fd63","permalink":"https://andleb.netlify.app/project/derivatives/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/derivatives/","section":"project","summary":" Derivatives pricing in modern C++. ","tags":["Quant","Finance","C++"],"title":"derivatives","type":"project"},{"authors":null,"categories":null,"content":"This is the thesis for my Diploma (Master’s and Bachelor’s combined) in Mathematical Physics at the University of Ljubljana. It was advised by dr. Tomaž Rejec from the Department of Theoretical Physics, Jožef Stefan Institute.\nThe work is an original examination of the effects that a coupled voltage probe has on the current flowing through a quantum dot. This was done by numerically simulating the problem from first principles (i.e. from a version of Schrödinger’s equation). The main challenge was to ensure that the voltage on the probe was set just so there was no current flowing in or out of it, while the system was being integrated - i.e. without knowing what the actual currents at that step were.\nThe document attached significantly compresses the theoretical background of the thesis (based on the Non-Equilibrium Green’s Functions formalism - NEGF), which is to be found in the referenced works since it falls outside of the Diploma program and the recommended Diploma thesis format.\n(University of Ljubljana Repository reference page)\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e37bb1733e6c4b525f35e615584a7b4c","permalink":"https://andleb.netlify.app/project/diploma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/diploma/","section":"project","summary":"Time-dependent current through a quantum dot in the presence of a voltage probe (in Slovene)","tags":["Mesoscopic transport","Nanophysics","NEGF"],"title":"Diploma Thesis in Mathematical Physics","type":"project"}]