<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diffusion | Andrej&#39;s thoughts</title>
    <link>https://andleb.netlify.app/tag/diffusion/</link>
      <atom:link href="https://andleb.netlify.app/tag/diffusion/index.xml" rel="self" type="application/rss+xml" />
    <description>Diffusion</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 02 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://andleb.netlify.app/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Diffusion</title>
      <link>https://andleb.netlify.app/tag/diffusion/</link>
    </image>
    
    <item>
      <title>Diffusion models: an introduction</title>
      <link>https://andleb.netlify.app/post/diffusion-models/</link>
      <pubDate>Sun, 02 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://andleb.netlify.app/post/diffusion-models/</guid>
      <description>


&lt;!-- LaTeX preamble --&gt;
&lt;p&gt;&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  
&lt;/details&gt;
&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Diffusion models seem to have taken the world by storm due to their amazing generative powers. That is not their only
advantage, however. With a bit of a tongue in the proverbial cheek, statistical methods can be traditionally classified
as either inflexible (classical stats), computationally expensive (MCMC), or non-analytical (boosted trees). From this
perspective, &lt;strong&gt;diffusion models&lt;/strong&gt; are a significant outlier since they are extremely flexible, provide access to the full
posterior (and conditional) distributions, and are computationally less expensive than many of the competing methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General structure&lt;/h2&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;generative-overview.png&#34; title=&#34;Overview of Generative models&#34; style=&#34;width:70.0%&#34; alt=&#34;An overview of generative models (source)&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;An overview of generative models&lt;/em&gt;
(&lt;a href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;&gt;source&lt;/a&gt;)&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Diffusion models are composed of two separate processes, &lt;strong&gt;the forward&lt;/strong&gt; and &lt;strong&gt;the backward&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;forward-diffusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forward diffusion&lt;/h3&gt;
&lt;p&gt;In general - a diffusion process can be characterized by a &lt;em&gt;Markov diffusion kernel&lt;/em&gt; given a final state &lt;span class=&#34;math inline&#34;&gt;\(\pi(x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    q(x_t | x_{t-1}) = T_\pi(x_t | x_{t-1}; \beta_t)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Since it’s a length-one Markov process, we have for the full joint: &lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    q(x_{t=0:T} ) = q(x_0) \prod_{i=1}^T q(x_i | x_{i-1})
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Usually, we use a Gaussian diffusion, for which the posterior is closed form (c.f. the 1-step update in &lt;em&gt;Kalman filtering&lt;/em&gt;):&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    q(x_t | x_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_t}x_{t-1}, \beta_t \mathbf{I})
\end{equation}\]&lt;/span&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt;s are called the &lt;strong&gt;variance schedule&lt;/strong&gt;, with usually &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 &amp;lt; \beta_2 &amp;lt; ... &amp;lt; \beta_T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, due to the property of the Gaussian distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\alpha_t := 1 - \beta_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{\alpha}_t := \prod_{s=1}^t \alpha_t\]&lt;/span&gt;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
q(x_t | x_0) = \mathcal{N}(\sqrt{\bar{\alpha_t}}~x_0, (1 - \alpha_t) \mathbf{I}),
\end{equation}\]&lt;/span&gt;
&lt;p&gt;meaning any state in the forward process can be expressed knowing &lt;em&gt;just the initial one&lt;/em&gt; and the &lt;em&gt;variance schedule&lt;/em&gt;. In
general, the theoretical underpinning of &lt;em&gt;Langevin dynamics&lt;/em&gt; guarantees any smooth distribution can be corrupted into
Gaussian noise, meaning the initial distribution can be almost arbitrarily complex, giving the model its expressive power.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reverse-diffusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reverse diffusion&lt;/h3&gt;
&lt;p&gt;The reverse process is characterized by a new transition probability &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;.
Its starting point is the stationary distribution at the final time &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    \pi(x_T) := p(x_T)
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Like before:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    p(x_{t=T:0}) = p(x_T) \prod_{i=T-1}^1 p(x_i+1 | x_{i})
\end{equation}\]&lt;/span&gt;
&lt;p&gt;For Gaussian (and binomial), the joint is still in the same family; however, there is no closed-form for the parameters &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;.
These need to be estimated, in this case via neural networks.&lt;/p&gt;
&lt;p&gt;A natural training target is the likelihood of the original data &lt;span class=&#34;math inline&#34;&gt;\(p(x_0)\)&lt;/span&gt; as given by the learned reverse process’ distribution,
obtained by marginalizing the full joint:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    p(x_0) = \int_{\mathcal{X}} p(x_{t=T:0}) ~ dx_1 .... dx_T
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is computationally intractable! A trick is to use annealed importance sampling - comparing the relative probability of the backward - &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; - and the forward trajectories &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;, where the latter are known in closed form:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    p(x_0) = \int_{\mathcal{X}} q(x_{t=1:T}) ~p(x_T)~ \prod_{t=1}^T \frac{p(x_{t-1}|x_t)}{q(x_{t}|x_{t-1})} ~ dx_1 .... dx_T
\end{equation}\]&lt;/span&gt;
&lt;p&gt;In the limit of very small &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;, both directions become the same.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;training&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;We maximize the &lt;strong&gt;expected&lt;/strong&gt; log likelihood of the original data &lt;span class=&#34;math inline&#34;&gt;\(p(x_0)\)&lt;/span&gt; under the original true distribution &lt;span class=&#34;math inline&#34;&gt;\(q(x_0)\)&lt;/span&gt;:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    &amp;amp; L(p, q) = \mathbb{E}_{q(x_0)} [p(x_0)] = \int_{\mathcal{X}} q(x_0) ~ \log p(x_T)~ dx_0\\
    &amp;amp; = -~\mathbb{E}_{q} \left[ \underbrace{D( q(x_T | x_0) || p(x_T))}_{L_T} + \underbrace{\sum_{t=2}^T D( q(x_{t-1} | x_t, x_0) || p(x_{t-1} || x_t))}_{L_{t-1}} - \underbrace{\log p(x_0 | x+1)}_{L_0}   \right]
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;It can be &lt;em&gt;lower bounded&lt;/em&gt; by a closed-form expression:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
    L \geq K = - \sum_{t=2}^T \int q(x_0, x_t) ~ D(q(x_{t-1}|x_{t}, x_{0}) ~||~ p(x_{t-1}|x_{t}))~dx_0,~dx_t\\
    + H_q(X_T|X_0) - H_q(X_1|X_0) - H_p(X_T),&amp;amp;
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the K-L divergence and H denotes the (conditional) entropies. Hence maximizing the latter maximizes the
former.&lt;/p&gt;
&lt;p&gt;Additionally, conditioning the forward process posteriors on &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; gives us the closed-forms&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
q(x_{t-1}|x_{t}, x_{0}) = \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t \mathbf{I})
\end{equation}\]&lt;/span&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde{\mu}_t := \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t}~x_0 + \frac{\sqrt{\bar{\alpha}_{t}} (1 - \bar{\alpha}_{t})}{1 - \bar{\alpha}_t}~x_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde{\beta}_t := \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The goal of training is therefore to &lt;strong&gt;estimate the reverse Markov transition densities&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(x_{t-1}|x_t)\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\hat{p}(x_{t-1}|x_t) = \underset{p}{\operatorname{argmax}} K\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As mentioned, in the case of the Gaussian and binomial, the reverse process stays in the same family, therefore the task amounts to
estimating the parameters.&lt;/p&gt;
&lt;div id=&#34;variance-schedule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance schedule&lt;/h3&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt; is a free parameter, it can be learned simultaneously with the whole &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; optimization procedure,
freezing the other variables and optimizing on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Alternatively, the first paper also treated a sequence as a
hyperparameter and used a simply linearly increasing &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. This is also the approach of &lt;em&gt;Ho et al.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-p&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Our goal is to learn the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x_{t-1} | x_t) = \mathcal{N}(\mu(x_t, t), \Sigma(x_t, t)), ~~ t \in [T, 0]
\]&lt;/span&gt;
which corresponds to estimating &lt;span class=&#34;math inline&#34;&gt;\(\mu(x_t, t), \Sigma(x_t, t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For the variance, it is simply set to be isotropic with the diagonal entries either &lt;strong&gt;fixed&lt;/strong&gt; at a constant (either
&lt;span class=&#34;math inline&#34;&gt;\(\beta_t\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\beta}_t\)&lt;/span&gt;) or &lt;strong&gt;learned&lt;/strong&gt;. The latter proved to be unstable in Ho et al., but has since been
successfully implemented.&lt;/p&gt;
&lt;p&gt;The mean is, of course, learned in all implementations and proceeds as follows: by inspecting the &lt;span class=&#34;math inline&#34;&gt;\(L_{t-1}\)&lt;/span&gt; term in the likelihood in &lt;a href=&#34;#training&#34;&gt;Training&lt;/a&gt;, the minimizing mean can be expressed by the reparametrization:
&lt;span class=&#34;math display&#34;&gt;\[x_t = \sqrt{\bar{\alpha}_t}~x_0 + \sqrt{1 - \bar{\alpha}_t}~\epsilon \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\epsilon \sim \mathcal{N}(0, \mathbf{I})\]&lt;/span&gt;&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\mu(x_t, t) = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha_t}}} \epsilon_{\theta}(x_t, t)),
\end{equation}\]&lt;/span&gt;
&lt;p&gt;meaning we instead learn the estimator of the &lt;strong&gt;noise&lt;/strong&gt; in the mean term at step &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{\theta}(x_t, t))\)&lt;/span&gt; from the state &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This can be further simplified by setting &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; to be &lt;em&gt;continuous&lt;/em&gt; on &lt;span class=&#34;math inline&#34;&gt;\([1, T]\)&lt;/span&gt; and optimizing the following simplified
objective for &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_\theta(x_t, t)\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
L_{\text{simple}}(\theta) := \mathbb{E}_{t, x_0, \epsilon} \left[\Vert \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \Vert^2\right]
\end{equation}\]&lt;/span&gt; This can be optimized using standard optimization techniques, such as gradient descent. We can now state
the two algorithms needed to first, train the noise estimator &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_\theta\)&lt;/span&gt;, and then, obtain a sample from the
reverse diffusion process: &lt;img src=&#34;ho_algos.png&#34; title=&#34;The two algorithms due to Ho et al.&#34; style=&#34;width:75.0%&#34; alt=&#34;Algorithms due to Ho et al.&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;architecture&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Architecture&lt;/h3&gt;
&lt;p&gt;Ho et al. used a variation of an U-net called &lt;a href=&#34;https://arxiv.org/abs/1701.05517&#34;&gt;PixelCNN++&lt;/a&gt; to estimate
&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{\theta}(x_t, t))\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;u-net-architecture.png&#34; title=&#34;The original U-net&#34; style=&#34;width:50.0%&#34; alt=&#34;The original U-net&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;The original U-net&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;pixelcnn.png&#34; title=&#34;Salimans et al. - PixelCNN++ structure&#34; style=&#34;width:125.0%&#34; alt=&#34;PixelCNN++ structure&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;PixelCNN++ structure&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-does-it-work-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How does it work in practice?&lt;/h2&gt;
&lt;div id=&#34;example---recovery&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example - recovery&lt;/h3&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;sohl-dickstein_spiral.png&#34; title=&#34;Spiral distribution recovery&#34; style=&#34;width:80.0%&#34; alt=&#34;Recovery (middle row) of a spiral distribution (top) using a Gaussian diffusion model. The bottom row represents the “drift term”, i.e. the field controlling the mean for the “particles” in the next step of the reverse process.&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;Recovery (middle row) of a spiral distribution (top) using a Gaussian diffusion model. The bottom row represents the
“drift term”, i.e. the field controlling the mean for the “particles” in the next step of the reverse
process.&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example---generation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example - generation&lt;/h3&gt;
&lt;p&gt;Ho et al. used the modified network for both &lt;strong&gt;conditional&lt;/strong&gt; and &lt;strong&gt;unconditional&lt;/strong&gt; generation.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;unconditional generation&lt;/em&gt; was performed by estimating &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt; - the end result of the reverse process - from
initially random &lt;span class=&#34;math inline&#34;&gt;\(x_T\)&lt;/span&gt;. The following figure shows the order in which features crystallize when the &lt;em&gt;initial state is
itself sampled from various points of the reverse process&lt;/em&gt;. The training dataset was CIFAR10.&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;ho_uncgen.png&#34; title=&#34;Unconditional generation from the reverse process sample&#34; width=&#34;1500&#34; alt=&#34;Unconditional generation from the reverse process sample&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;Unconditional generation from the reverse process
sample&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For &lt;em&gt;conditional generation,&lt;/em&gt; the authors selected a &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt; from the actual distribution and sampled from the predictive
posterior &lt;span class=&#34;math inline&#34;&gt;\(p(x_0|x_t)\)&lt;/span&gt;. The following figure shows the results conditioned on the bottom-right quadrant, for a network
trained on CelebA-HQ:&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;ho_condgen.png&#34; title=&#34;Conditional generation samples from a given state&#34; width=&#34;1500&#34; alt=&#34;Conditional generation samples from a given state: the earlier the x_t, the more deterministic the outcome.&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;Conditional generation samples from a given state:&lt;/em&gt; the earlier the &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt;, the more deterministic the
outcome.&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;imagen-dall-e-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Imagen &amp;amp; DALL-E 2&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;Ho and Salimans&lt;/a&gt; improved the above procedure by introducing the notion of &lt;em&gt;guiding&lt;/em&gt;
the model during training on labeled data, i.e. estimating &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_\theta(x_t, t ∣ y)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are the labels. The
crux of the approach is training both on conditional and unconditional objectives at once by randomly setting the class
label to a null class with some predetermined probability. Likewise, the samples are drawn from a convex combination
(with the same coefficient) of both &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-s.&lt;/p&gt;
&lt;p&gt;This was used by Nichol et al. in &lt;a href=&#34;https://arxiv.org/abs/2112.10741?s=09&#34;&gt;GLIDE&lt;/a&gt;, which uses information extracted from
text to do the guiding, combining a transformer with the previously described architecture.&lt;/p&gt;
&lt;p&gt;This approach has been used to construct &lt;a href=&#34;https://arxiv.org/pdf/2205.11487.pdf&#34;&gt;Imagen&lt;/a&gt;, which uses additional diffusion
models to up-sample the image created by the guided diffusion process. The text embeddings are provided by a
&lt;em&gt;pretrained&lt;/em&gt; transformer’s encoder.&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;imagen.png&#34; width=&#34;700&#34; alt=&#34;Imagen structure&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;Imagen structure&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The other diffusion-based model to make waves recently - DALL-E 2 - uses a bit more complex approach:&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;dalle2.png&#34; title=&#34;DALL-E 2 architecture&#34; alt=&#34;DALL-E 2 architecture&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;DALL-E 2 architecture&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;First, it re-uses a model called CLIP (Contrastive Language-Image Pre-training) to construct a &lt;em&gt;mapping between captions
and images&lt;/em&gt; (top of the schematic above). In practice, the net result of this is a &lt;em&gt;joint embedding of text and images&lt;/em&gt;
in a representation space.&lt;/p&gt;
&lt;p&gt;In generation, this model is frozen and a version of the GLIDE guided diffusion model is used to generate images
&lt;strong&gt;starting from the image representation space&lt;/strong&gt; (as opposed to random noise, for instance). As previously, additional
up-samplers are used in the decoder, as well.&lt;/p&gt;
&lt;p&gt;To generate from text prompts, we need to map the caption text embeddings to the above mentioned image embeddings, which
are the starting point for the decoder. This is done with an &lt;em&gt;additional diffusion model&lt;/em&gt; called &lt;strong&gt;the prior&lt;/strong&gt;, which
generates &lt;em&gt;multiple possible embeddings.&lt;/em&gt; In other words, this is a generative model of the image embeddings given the
text embeddings. The prior trains a decoder-only transformer to predict the conditional reverse process, as opposed to the U-net
used in other examples.&lt;/p&gt;
&lt;div class=&#34;float&#34;&gt;
&lt;img src=&#34;dalle2_interpolation.png&#34; title=&#34;DALL-E 2: interpolation of the image embedding space&#34; alt=&#34;DALL-E 2: interpolation of the image embedding space by the decoder&#34; /&gt;
&lt;div class=&#34;figcaption&#34;&gt;&lt;em&gt;DALL-E 2: interpolation of the image embedding
space by the decoder&lt;/em&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The original paper: &lt;a href=&#34;https://arxiv.org/abs/1503.03585&#34;&gt;Sohl-Dickstein et al. - Deep Unsupervised Learning using Non-equilibrium
Thermodynamics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Performant implementation: &lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;Ho et al. - Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A good blog post: &lt;a href=&#34;https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&#34;&gt;Weng, Lilian. (Jul 2021). What are diffusion models?
Lil’Log.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A summary of recent developments:
&lt;a href=&#34;https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html&#34; class=&#34;uri&#34;&gt;https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DALL-E 2 initial paper: &lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;Ramesh et al. - Hierarchical Text-Conditional Image Generation with CLIP
Latents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Related:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kalman_filter&#34;&gt;The Kalman filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../uploads/leban_andrej%20-%20continuous%20time%20kalman%20filter.pdf&#34;&gt;My presentation on the Continuous-time Kalman
filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;&gt;Blei, Kucukelbir, McAuliffe - Variational Inference: A Review for Statisticians&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
